<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>devinterp.optim package &mdash; devinterp 0.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=92fd9be5" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=938c9ccc"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/custom.js"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]]}})</script>
        <script>window.MathJax = {"tex": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]]}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="devinterp.slt package" href="devinterp.slt.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            devinterp
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Welcome to devinterp’s documentation!</a></li>
<li class="toctree-l1"><a class="reference internal" href="devinterp.slt.html">SLT Observables</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Sampling Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-devinterp.optim.sgld">devinterp.optim.sgld module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#devinterp.optim.sgld.SGLD"><code class="docutils literal notranslate"><span class="pre">SGLD</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-devinterp.optim.sgnht">devinterp.optim.sgnht module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#devinterp.optim.sgnht.SGNHT"><code class="docutils literal notranslate"><span class="pre">SGNHT</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-devinterp.optim">Module contents</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">devinterp</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">devinterp.optim package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/devinterp.optim.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="devinterp-optim-package">
<h1>devinterp.optim package<a class="headerlink" href="#devinterp-optim-package" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-devinterp.optim.sgld">
<span id="devinterp-optim-sgld-module"></span><h2>devinterp.optim.sgld module<a class="headerlink" href="#module-devinterp.optim.sgld" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="devinterp.optim.sgld.SGLD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">devinterp.optim.sgld.</span></span><span class="sig-name descname"><span class="pre">SGLD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">localization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounding_box_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_noise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_mala_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/devinterp/optim/sgld.html#SGLD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#devinterp.optim.sgld.SGLD" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></p>
<p>Implements Stochastic Gradient Langevin Dynamics (SGLD) optimizer.</p>
<p>This optimizer blends Stochastic Gradient Descent (SGD) with Langevin Dynamics,
introducing Gaussian noise to the gradient updates. This makes it sample weights from the posterior distribution, instead of optimizing weights.</p>
<p>This implementation follows Lau et al.’s (2023) implementation, which is a modification of
Welling and Teh (2011) that omits the learning rate schedule and introduces
an localization term that pulls the weights towards their initial values.</p>
<p>The equation for the update is as follows:</p>
<p><div class="math notranslate nohighlight">
\[\Delta w_t = \frac{\epsilon}{2}\left(\frac{\beta n}{m} \sum_{i=1}^m \nabla \log p\left(y_{l_i} \mid x_{l_i}, w_t\right)+\gamma\left(w_0-w_t\right) - \lambda w_t\right) + N(0, \epsilon\sigma^2)\]</div>
</p>
<p>where <span class="math notranslate nohighlight">\(w_t\)</span> is the weight at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\epsilon\)</span> is the learning rate,
<span class="math notranslate nohighlight">\((\beta n)\)</span> is the inverse temperature (we’re in the tempered Bayes paradigm),
<span class="math notranslate nohighlight">\(n\)</span> is the number of training samples, <span class="math notranslate nohighlight">\(m\)</span> is the batch size, <span class="math notranslate nohighlight">\(\gamma\)</span> is
the localization strength, <span class="math notranslate nohighlight">\(\lambda\)</span> is the weight decay strength,
and <span class="math notranslate nohighlight">\(\sigma\)</span> is the noise term.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGLD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">optimal_temperature</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><code class="code highlight python docutils literal highlight-python"><span class="n">localization</span></code> is unique to this class and serves to guide the weights towards their original values. This is useful for estimating quantities over the local posterior.</p></li>
<li><p><code class="code highlight python docutils literal highlight-python"><span class="n">noise_level</span></code> is not intended to be changed, except when testing! Doing so will raise a warning.</p></li>
<li><p>Although this class is a subclass of <code class="code highlight python docutils literal highlight-python"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span></code>, this is a bit of a misnomer in this case. It’s not used for optimizing in LLC estimation, but rather for sampling from the posterior distribution around a point.</p></li>
<li><p>Hyperparameter optimization is more of an art than a science. Check out <a class="reference external" href="https://www.github.com/timaeus-research/devinterp/blob/main/examples/sgld_calibration.ipynb">the calibration notebook</a> <a class="reference external" href="https://colab.research.google.com/github/timaeus-research/devinterp/blob/main/examples/sgld_calibration.ipynb"><img alt="colab6" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> for how to go about it in a simple case.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Iterable</em>) – Iterable of parameters to optimize or dicts defining parameter groups. Either <code class="code highlight python docutils literal highlight-python"><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span></code> or something more fancy, just like other <code class="code highlight python docutils literal highlight-python"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span></code> classes.</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – Learning rate <span class="math notranslate nohighlight">\(\epsilon\)</span>. Default is 0.01</p></li>
<li><p><strong>noise_level</strong> (<em>float</em><em>, </em><em>optional</em>) – Amount of Gaussian noise <span class="math notranslate nohighlight">\(\sigma\)</span> introduced into gradient updates. Don’t change this unless you know very well what you’re doing! Default is 1</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – L2 regularization term <span class="math notranslate nohighlight">\(\lambda\)</span>, applied as weight decay. Default is 0</p></li>
<li><p><strong>localization</strong> (<em>float</em><em>, </em><em>optional</em>) – Strength of the force <span class="math notranslate nohighlight">\(\gamma\)</span> pulling weights back to their initial values. Default is 0</p></li>
<li><p><strong>bounding_box_size</strong> (<em>float</em><em>, </em><em>optional</em>) – the size of the bounding box enclosing our trajectory. Default is None</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Temperature, float (default: 1., set by sample() to utils.optimal_temperature(dataloader)=len(batch_size)/np.log(len(batch_size)))</p></li>
<li><p><strong>save_noise</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to store the per-parameter noise during optimization. Default is False</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>Warning</strong> – if <code class="code highlight python docutils literal highlight-python"><span class="n">noise_level</span></code> is set to anything other than 1</p></li>
<li><p><strong>Warning</strong> – if <code class="code highlight python docutils literal highlight-python"><span class="n">temperature</span></code> is set to 1</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-devinterp.optim.sgnht">
<span id="devinterp-optim-sgnht-module"></span><h2>devinterp.optim.sgnht module<a class="headerlink" href="#module-devinterp.optim.sgnht" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="devinterp.optim.sgnht.SGNHT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">devinterp.optim.sgnht.</span></span><span class="sig-name descname"><span class="pre">SGNHT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diffusion_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bounding_box_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_noise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_mala_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/devinterp/optim/sgnht.html#SGNHT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#devinterp.optim.sgnht.SGNHT" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></p>
<p>Implement the Stochastic Gradient Nose Hoover Thermostat (SGNHT) Optimizer.
This optimizer blends SGD with an adaptive thermostat variable to control the magnitude of the injected noise,
maintaining the kinetic energy of the system.</p>
<p>It follows Ding et al.’s (2014) implementation.</p>
<p>The equations for the update are as follows:</p>
<p><div class="math notranslate nohighlight">
\[\Delta w_t = \epsilon\left(\frac{\beta n}{m} \sum_{i=1}^m \nabla \log p\left(y_{l_i} \mid x_{l_i}, w_t\right) - \xi_t w_t \right) + \sqrt{2A} N(0, \epsilon)\]</div>

<div class="math notranslate nohighlight">
\[\Delta\xi_{t} = \epsilon \left( \frac{1}{n} \|w_t\|^2 - 1 \right)\]</div>
</p>
<p>where <span class="math notranslate nohighlight">\(w_t\)</span> is the weight at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\epsilon\)</span> is the learning rate,
<span class="math notranslate nohighlight">\((\beta n)\)</span> is the inverse temperature (we’re in the tempered Bayes paradigm),
<span class="math notranslate nohighlight">\(n\)</span> is the number of samples, <span class="math notranslate nohighlight">\(m\)</span> is the batch size,
<span class="math notranslate nohighlight">\(\xi_t\)</span> is the thermostat variable at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(A\)</span> is the diffusion factor,
and <span class="math notranslate nohighlight">\(N(0, A)\)</span> represents Gaussian noise with mean 0 and variance <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><code class="code highlight python docutils literal highlight-python"><span class="n">diffusion_factor</span></code> is unique to this class, and functions as a way to allow for random parameter changes while keeping them from blowing up by guiding parameters back to a slowly-changing thermostat value using a friction term.</p></li>
<li><p>This class does not have an explicit localization term like <a class="reference internal" href="#devinterp.optim.sgld.SGLD" title="devinterp.optim.sgld.SGLD"><code class="xref py py-func docutils literal notranslate"><span class="pre">SGLD()</span></code></a> does. If you want to constrain your sampling, use <code class="code highlight python docutils literal highlight-python"><span class="n">bounding_box_size</span></code></p></li>
<li><p>Although this class is a subclass of <code class="code highlight python docutils literal highlight-python"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span></code>, this is a bit of a misnomer in this case. It’s not used for optimizing in LLC estimation, but rather for sampling from the posterior distribution around a point.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Iterable</em>) – Iterable of parameters to optimize or dicts defining parameter groups. Either <code class="code highlight python docutils literal highlight-python"><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span></code> or something more fancy, just like other <code class="code highlight python docutils literal highlight-python"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span></code> classes.</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – Learning rate <span class="math notranslate nohighlight">\(\epsilon\)</span>. Default is 0.01</p></li>
<li><p><strong>diffusion_factor</strong> (<em>float</em><em>, </em><em>optional</em>) – The diffusion factor <span class="math notranslate nohighlight">\(A\)</span> of the thermostat. Default is 0.01</p></li>
<li><p><strong>bounding_box_size</strong> (<em>float</em><em>, </em><em>optional</em>) – the size of the bounding box enclosing our trajectory. Default is None</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Temperature, float (default: 1., set by sample() to utils.optimal_temperature(dataloader)=len(batch_size)/np.log(len(batch_size)))</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>Warning</strong> – if <code class="code highlight python docutils literal highlight-python"><span class="n">temperature</span></code> is set to 1</p></li>
<li><p><strong>Warning</strong> – if <code class="code highlight python docutils literal highlight-python"><span class="n">NoiseNorm</span></code> callback is used</p></li>
<li><p><strong>Warning</strong> – if <code class="code highlight python docutils literal highlight-python"><span class="n">MALA</span></code> callback is used</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-devinterp.optim">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-devinterp.optim" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="devinterp.slt.html" class="btn btn-neutral float-left" title="devinterp.slt package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Van Wingerden et al..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>