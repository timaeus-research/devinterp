{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Any\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"/home/paperspace/devinterp/\")\n",
    "from devinterp.zoo.arithmetic import ModularArithmeticConfig, ModularArithmetic\n",
    "from devinterp.zoo.transformer import TransformerConfig, Transformer\n",
    "from devinterp.slt.sampler import estimate_rlct\n",
    "\n",
    "from devinfra.learner import LearnerConfig, Learner\n",
    "from devinfra.utils.device import get_default_device\n",
    "from devinfra.utils.tensors import Reduction\n",
    "from devinfra.evals import CombineEvaluators, Evaluator, RepeatEvaluator\n",
    "from devinfra.optim.schedulers import LRScheduler\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "MODULUS = 113\n",
    "\n",
    "trainset, testset = ModularArithmeticConfig(\n",
    "    operator=\"/\",\n",
    "    modulus=MODULUS,\n",
    "    seed=0,\n",
    "    split=0.4\n",
    ").factory_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evals\n",
    "\n",
    "def cross_entropy_last_token(outputs, targets, reduction: Reduction = \"sum\"):\n",
    "    \"\"\"\n",
    "    Wrapper around cross entropy loss because we only care about the last number predicted.\n",
    "    \"\"\"\n",
    "    # Only look at predictions of last numbers\n",
    "    outputs = outputs[:, -1]\n",
    "\n",
    "    # Compute individual and summed losses for final number\n",
    "    logprobs = F.log_softmax(outputs.to(torch.float32), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=targets.unsqueeze(1), dim=-1)\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = -torch.mean(prediction_logprobs)\n",
    "    elif reduction == \"sum\":\n",
    "        loss = -torch.sum(prediction_logprobs)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid reduction argument.\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1024, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024, shuffle=False)\n",
    "\n",
    "\n",
    "def eval_loss_and_acc(model: nn.Module, *_) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, loader in zip([\"train\", \"test\"], [trainloader, testloader]):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(x)\n",
    "\n",
    "            total += cross_entropy_last_token(y_hat, y, reduction=\"sum\").item()\n",
    "            correct += (y_hat[:, -1, :].max(dim=1).indices == y).sum().item()  # argmax doesn't work for device=mps\n",
    "\n",
    "        results[f\"{name}/loss\"] = total / len(loader.dataset)\n",
    "        results[f\"{name}/accuracy\"] = correct / len(loader.dataset)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_rlct(model: nn.Module, *_):\n",
    "    optimizer_kwargs = dict(\n",
    "        lr=1e-7, noise_level=1., weight_decay=3e-7, elasticity=10., temperature=\"adaptive\", num_samples=len(trainset)\n",
    "    )\n",
    "    return {\n",
    "        \"rlct\": estimate_rlct(model, trainloader, cross_entropy_last_token, 'sgld', optimizer_kwargs, num_draws=20, num_chains=5, num_burnin_steps=0, num_steps_bw_draws=1, cores=1, pbar=False)\n",
    "    }\n",
    "\n",
    "\n",
    "evals = CombineEvaluators([\n",
    "    eval_loss_and_acc,\n",
    "    RepeatEvaluator(eval_rlct, 5),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_samples=5107 batch_size=256 run_name=None num_steps=25000 logger_config=MetricLoggingConfig(project=None, entity=None, logging_steps=(0...100) 5 steps, out_file=None, use_df=False, stdout=False, run_id=None) checkpointer_config=CheckpointerConfig(bucket_name=None, project_dir=div-mod-113, local_root=../) optimizer_config=OptimizerConfig(optimizer_type='AdamW', lr=0.001, weight_decay=0.2, momentum=None, betas=(0.9, 0.98), noise_level=None, elasticity=None, temperature=None, num_samples=None) scheduler_config=None device=device(type='cuda') criterion='cross_entropy'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_config = TransformerConfig(d_vocab=MODULUS + 1)\n",
    "model = model_config.factory().to(device)\n",
    "\n",
    "learner_config = LearnerConfig(\n",
    "    num_training_samples=len(trainset),\n",
    "    batch_size=256,\n",
    "    num_steps=25_000,\n",
    "    criterion=\"cross_entropy\",\n",
    "    device=device,\n",
    "    optimizer_config={\n",
    "        \"optimizer_type\": \"AdamW\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0.2,\n",
    "        \"betas\":(0.9, 0.98),\n",
    "    },\n",
    "    logger_config={\n",
    "        # \"project\": \"grokking\",\n",
    "        # \"entity\": \"devinterp\",\n",
    "        \"logging_steps\": {\n",
    "            \"log_space\": (1, 25, 1),\n",
    "            \"linear_space\":  (0, 100, 4),\n",
    "        },\n",
    "        \"use_std\": True\n",
    "   },\n",
    "   checkpointer_config={\n",
    "        \"checkpoint_steps\": {\n",
    "            \"log_space\": (1, 25, 1),\n",
    "            \"linear_space\":  (0, 100, 4),\n",
    "        },\n",
    "        # \"bucket\": \"devinterp\",\n",
    "        \"project_dir\": \"div-mod-113\",\n",
    "        \"local_root\": \"../\"\n",
    "   },\n",
    ")\n",
    "learner = Learner(\n",
    "    model=model,\n",
    "    dataset=trainset,\n",
    "    evaluator=evals,\n",
    "    config=learner_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e844523cd3a34acd8db22ea8dba2c2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [256, 114], got [256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/paperspace/devinterp/demos/grokking.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bothello/home/paperspace/devinterp/demos/grokking.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# learner.save_checkpoint(0)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bothello/home/paperspace/devinterp/demos/grokking.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m learner\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/devinfra/devinfra/learner.py:152\u001b[0m, in \u001b[0;36mLearner.train\u001b[0;34m(self, resume, verbose)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    151\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)\n\u001b[0;32m--> 152\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(y_hat, y)\n\u001b[1;32m    153\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/devinterp/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [256, 114], got [256]"
     ]
    }
   ],
   "source": [
    "# learner.save_checkpoint(0)\n",
    "learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
