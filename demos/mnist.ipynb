{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(1, \"c:\\\\Users\\\\svwin\\\\New folder\\\\devinterp\") # TODO fix path\n",
    "\n",
    "from devinterp.slt.sampler import Sampler, SamplerConfig, estimate_rlct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "    return test_loss / len(test_loader)\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SamplerConfig\ncriterion\n  Input should be callable [type=callable_type, input_value='cross_entropy', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.1/v/callable_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     10\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m sampler_config \u001b[39m=\u001b[39m SamplerConfig(\n\u001b[0;32m     12\u001b[0m     optimizer_config\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(\n\u001b[0;32m     13\u001b[0m         optimizer_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSGLD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m         lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[0;32m     15\u001b[0m         noise_level\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m     16\u001b[0m         weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.\u001b[39;49m,\n\u001b[0;32m     17\u001b[0m         elasticity\u001b[39m=\u001b[39;49m\u001b[39m1.\u001b[39;49m,\n\u001b[0;32m     18\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39madaptive\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m         num_samples\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     20\u001b[0m     ),\n\u001b[0;32m     21\u001b[0m     num_chains\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     22\u001b[0m     num_draws_per_chain\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m     23\u001b[0m     num_burnin_steps\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     24\u001b[0m     num_steps_bw_draws\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     25\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     26\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,        \n\u001b[0;32m     27\u001b[0m     criterion \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcross_entropy\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m# alternatives: mse\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m sampler \u001b[39m=\u001b[39m Sampler(model, train_data, sampler_config)\n",
      "File \u001b[1;32mc:\\Users\\svwin\\New folder\\devinterp\\.venv\\lib\\site-packages\\pydantic\\main.py:159\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    158\u001b[0m __tracebackhide__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m __pydantic_self__\u001b[39m.\u001b[39;49m__pydantic_validator__\u001b[39m.\u001b[39;49mvalidate_python(data, self_instance\u001b[39m=\u001b[39;49m__pydantic_self__)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SamplerConfig\ncriterion\n  Input should be callable [type=callable_type, input_value='cross_entropy', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.1/v/callable_type"
     ]
    }
   ],
   "source": [
    "# Load MNIST data\n",
    "train_data = datasets.MNIST('./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# Load test data\n",
    "test_data = datasets.MNIST('./data', train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "# Initialize model, loss, optimizer and sgld sampler\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "sampler_config = SamplerConfig(\n",
    "    optimizer_config=dict(\n",
    "        optimizer_type=\"SGLD\",\n",
    "        lr=0.001,\n",
    "        noise_level=0.5,\n",
    "        weight_decay=0.,\n",
    "        elasticity=1.,\n",
    "        temperature='adaptive',\n",
    "        num_samples=100,\n",
    "    ),\n",
    "    num_chains=50,\n",
    "    num_draws_per_chain=20,\n",
    "    num_burnin_steps=0,\n",
    "    num_steps_bw_draws=1,\n",
    "    verbose=False,\n",
    "    batch_size=256,        \n",
    "    criterion = 'cross_entropy' # alternatives: mse\n",
    ")\n",
    "sampler = Sampler(model, train_data, sampler_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:08<00:00, 219.14it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'reduction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m train_loss \u001b[39m=\u001b[39m train_one_epoch(model, train_loader, optimizer, criterion)\n\u001b[0;32m      7\u001b[0m test_loss \u001b[39m=\u001b[39m evaluate(model, test_loader, criterion)\n\u001b[1;32m----> 8\u001b[0m rlct_estimate \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49msample(summary_fn\u001b[39m=\u001b[39;49mestimate_rlct)\n\u001b[0;32m     10\u001b[0m train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     11\u001b[0m test_losses\u001b[39m.\u001b[39mappend(test_loss)\n",
      "File \u001b[1;32mc:\\Users\\svwin\\New folder\\devinterp\\devinterp\\slt\\sampler.py:101\u001b[0m, in \u001b[0;36mSampler.sample\u001b[1;34m(self, kwargs, summary_fn)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m j, model \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mensemble):\n\u001b[0;32m    100\u001b[0m     yhats \u001b[39m=\u001b[39m model(xs)\n\u001b[1;32m--> 101\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(yhats, ys, reduction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnone\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    102\u001b[0m     loss \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n\u001b[0;32m    103\u001b[0m     loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\svwin\\New folder\\devinterp\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'reduction'"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "rlct_estimates = []\n",
    "for epoch in range(40):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    test_loss = evaluate(model, test_loader, criterion)\n",
    "    rlct_estimate = sampler.sample(summary_fn=estimate_rlct)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    rlct_estimates.append(rlct_estimate)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Test Loss: {test_loss}, RLCT estimate: {rlct_estimate}\")\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='tab:blue')\n",
    "ax1.plot(train_losses, label='Train Loss', color='tab:blue')\n",
    "ax1.plot(test_losses, label='Test Loss', color='tab:orange')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('rlct_estimate', color='tab:green')\n",
    "ax2.plot(rlct_estimates, label='rlct_estimate', color='tab:green')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
