{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timaeus-research/devinterp/blob/main/examples/grokking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: devinterp in /home/svwin/aether/.venv/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: seaborn in /home/svwin/aether/.venv/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: einops>=0.6.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (0.8.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.5 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (3.7.5)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (2.0.3)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (1.10.1)\n",
      "Requirement already satisfied: torch>=2.0.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (4.66.5)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from devinterp) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from matplotlib>=3.7.5->devinterp) (6.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from pandas>=1.5.3->devinterp) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from pandas>=1.5.3->devinterp) (2024.1)\n",
      "Requirement already satisfied: filelock in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from torch>=2.0.1->devinterp) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.1->devinterp) (12.6.68)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.7.5->devinterp) (3.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->devinterp) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from jinja2->torch>=2.0.1->devinterp) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/svwin/aether/.venv/lib/python3.8/site-packages (from sympy->torch>=2.0.1->devinterp) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install devinterp seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svwin/aether/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary\n",
    "from devinterp.utils import make_evaluate\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "\n",
    "\n",
    "sns.set_palette(\"deep\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "CHECKPOINTS_PATH = Path(\"./checkpoints/grokking\")\n",
    "if not os.path.exists(CHECKPOINTS_PATH):\n",
    "    os.makedirs(CHECKPOINTS_PATH)\n",
    "\n",
    "PRIMARY, SECONDARY, TERTIARY = sns.color_palette(\"deep\")[:3]\n",
    "PRIMARY_LIGHT, SECONDARY_LIGHT, TERTIARY_LIGHT = sns.color_palette(\"muted\")[:3]\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"USE_TPU_BACKEND\"] = \"1\"\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "DEVICE = xm.xla_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentParams:\n",
    "    p: int = 101\n",
    "    n_batches: int = 1500\n",
    "    n_save_model_checkpoints: int = 25\n",
    "    print_times: int = 25\n",
    "    lr: float = 0.005\n",
    "    batch_size: int = 16\n",
    "    hidden_size: int = 16\n",
    "    embed_dim: int = 16\n",
    "    train_frac: float = 0.2\n",
    "    random_seed: int = 0\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    weight_decay: float = 0.0002\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(params.p, params.embed_dim)\n",
    "        self.linear1r = nn.Linear(params.embed_dim, params.hidden_size, bias=True)\n",
    "        self.linear1l = nn.Linear(params.embed_dim, params.hidden_size, bias=True)\n",
    "        self.linear2 = nn.Linear(params.hidden_size, params.p, bias=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.vocab_size = params.p\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        x1 = self.embedding(a)\n",
    "        x2 = self.embedding(b)\n",
    "        x1 = self.linear1l(x1)\n",
    "        x2 = self.linear1r(x2)\n",
    "        x = x1 + x2\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test(model, dataset, device):\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for (x1, x2), y in dataset:\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            out = model(x1, x2)\n",
    "            loss = loss_fn(out, y)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(out)\n",
    "            if pred == y:\n",
    "                n_correct += 1\n",
    "    return n_correct / len(dataset), total_loss / len(dataset)\n",
    "\n",
    "\n",
    "def train(train_dataset, test_dataset, params):\n",
    "    model = MLP(params).to(params.device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=params.weight_decay, lr=params.lr\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "    print_every = params.n_batches // params.print_times\n",
    "    checkpoint_every = None\n",
    "    if params.n_save_model_checkpoints > 0:\n",
    "        checkpoint_every = params.n_batches // params.n_save_model_checkpoints\n",
    "\n",
    "    avg_loss = 0\n",
    "    loss_data = []\n",
    "    pbar = tqdm(total=params.n_batches, desc=\"Training\")\n",
    "    for i in range(params.n_batches):\n",
    "        # Sample random batch of data\n",
    "        batch = next(iter(train_loader))\n",
    "        (X_1, X_2), Y = batch\n",
    "        X_1, X_2, Y = X_1.to(params.device), X_2.to(params.device), Y.to(params.device)\n",
    "\n",
    "        # Gradient update\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_1, X_2)\n",
    "        loss = loss_fn(out, Y)\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if checkpoint_every and (i + 1) % checkpoint_every == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f\"{CHECKPOINTS_PATH}/checkpoint_batch_{i + 1}.pt\"\n",
    "            )\n",
    "\n",
    "        if (i + 1) % print_every == 0:\n",
    "            avg_loss /= print_every\n",
    "            val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "            loss_data.append(\n",
    "                {\n",
    "                    \"batch\": i + 1,\n",
    "                    \"train_loss\": avg_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_acc\": val_acc,\n",
    "                }\n",
    "            )\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"batch\": f\"{i + 1}/{params.n_batches}\",\n",
    "                    \"train_loss\": f\"{avg_loss:.4f}\",\n",
    "                    \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                    \"val_acc\": f\"{val_acc:.4f}\",\n",
    "                }\n",
    "            )\n",
    "            pbar.update(print_every)\n",
    "            avg_loss = 0\n",
    "\n",
    "    pbar.close()\n",
    "    df = pd.DataFrame(loss_data)\n",
    "    val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "    print(f\"Final Val Acc: {val_acc:.4f} | Final Val Loss: {val_loss:.4f}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def deterministic_shuffle(lst, seed):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lst)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def get_all_pairs(p):\n",
    "    pairs = []\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            pairs.append((i, j))\n",
    "    return set(pairs)\n",
    "\n",
    "\n",
    "# def make_random_dataset(p, seed):\n",
    "#     data = []\n",
    "#     pairs = get_all_pairs(p)\n",
    "#     for a, b in pairs:\n",
    "#         out = 2 * a * p + b\n",
    "#         out = hash_with_seed(out, seed) % p\n",
    "#         data.append(((torch.tensor(a), torch.tensor(b)), torch.tensor(out)))\n",
    "#     return data\n",
    "\n",
    "\n",
    "def make_dataset(p):\n",
    "    data = []\n",
    "    pairs = get_all_pairs(p)\n",
    "    for a, b in pairs:\n",
    "        out = 2 * a * p + b\n",
    "        out = out % p\n",
    "        data.append(((torch.tensor(a), torch.tensor(b)), torch.tensor(out)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def hash_with_seed(value, seed):\n",
    "    m = hashlib.sha256()\n",
    "    m.update(str(seed).encode(\"utf-8\"))\n",
    "    m.update(str(value).encode(\"utf-8\"))\n",
    "    return int(m.hexdigest(), 16)\n",
    "\n",
    "\n",
    "def train_test_split(dataset, train_split_proportion, seed):\n",
    "    l = len(dataset)\n",
    "    train_len = int(train_split_proportion * l)\n",
    "    idx = list(range(l))\n",
    "    idx = deterministic_shuffle(idx, seed)\n",
    "    train_idx = idx[:train_len]\n",
    "    test_idx = idx[train_len:]\n",
    "    return [dataset[i] for i in train_idx], [dataset[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 360/1500 [00:08<00:27, 41.61it/s, batch=360/1500, train_loss=0.0518, val_loss=0.0431, val_acc=1.0000]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m make_dataset(params\u001b[38;5;241m.\u001b[39mp)\n\u001b[1;32m      5\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m train_test_split(dataset, params\u001b[38;5;241m.\u001b[39mtrain_frac, params\u001b[38;5;241m.\u001b[39mrandom_seed)\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[90], line 102\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataset, test_dataset, params)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    101\u001b[0m     avg_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m print_every\n\u001b[0;32m--> 102\u001b[0m     val_acc, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     loss_data\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    104\u001b[0m         {\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m         }\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    111\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[1;32m    112\u001b[0m         {\n\u001b[1;32m    113\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;241m.\u001b[39mn_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m         }\n\u001b[1;32m    118\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[90], line 55\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, dataset, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x1, x2), y \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m     54\u001b[0m     x1, x2, y \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mto(device), x2\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 55\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(out, y)\n\u001b[1;32m     57\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/aether/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aether/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[90], line 42\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     40\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1r(x2)\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m+\u001b[39m x2\n\u001b[0;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/aether/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aether/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/aether/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:682\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 360/1500 [00:27<00:27, 41.61it/s, batch=360/1500, train_loss=0.0518, val_loss=0.0431, val_acc=1.0000]"
     ]
    }
   ],
   "source": [
    "params = ExperimentParams()\n",
    "torch.manual_seed(params.random_seed)\n",
    "\n",
    "dataset = make_dataset(params.p)\n",
    "train_data, test_data = train_test_split(dataset, params.train_frac, params.random_seed)\n",
    "\n",
    "df = train(train_dataset=train_data, test_dataset=test_data, params=params)\n",
    "plt.plot(df[\"val_loss\"])\n",
    "plt.plot(df[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLCT estimation hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(grid_search, \"../data/grokking-rlct-sweep.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
